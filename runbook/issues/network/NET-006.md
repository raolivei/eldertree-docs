---
id: NET-006
title: Tailscale Routing Table Conflict with k3s CNI
category: network
severity: critical
symptoms:
  - Pods cannot reach services on other nodes
  - DNS resolution failing inside pods
  - Cross-node communication broken
  - Connection timeouts to pod/service IPs
---

# NET-006: Tailscale Routing Table Conflict with k3s CNI

## Error Messages (Searchable)

```
Temporary failure in name resolution
Connection timed out
dial tcp 10.43.x.x:5432: i/o timeout
cannot connect to postgres-service.swimto.svc.cluster.local
DNS resolution failed for postgres-service
```

## Symptoms

- Pods on node-2 cannot reach pods/services on other nodes
- DNS resolution fails inside pods: `[Errno -3] Temporary failure in name resolution`
- `kubectl exec` works, but pod cannot reach external services or other pods
- PostgreSQL connection timeouts when API and DB are on different nodes
- Works perfectly when pods are scheduled on the same node
- CoreDNS pods may restart or show connection issues

## Root Cause

Tailscale adds routes to routing table 52 that intercept k3s pod and service network traffic:

```
10.42.0.0/16 dev tailscale0  # Pod network - WRONG!
10.43.0.0/16 dev tailscale0  # Service network - WRONG!
```

These routes cause traffic destined for k3s pods (10.42.0.0/16) and services (10.43.0.0/16) to be sent through the Tailscale interface instead of the Flannel VXLAN overlay network.

**Why this happens:**
- Tailscale advertises routes for the local network
- When k3s starts, it assigns pod CIDRs that Tailscale later claims
- Policy-based routing (table 52) has higher priority than main routing table
- Traffic that should go through `flannel.1` goes through `tailscale0` instead

## Diagnosis

### Step 1: Verify the Problem

```bash
# SSH to the affected node (usually node-2)
ssh raolivei@node-2.eldertree.local

# Check routing table 52 for conflicting routes
ip route show table 52 | grep -E "10\.4[23]"

# If you see these, Tailscale is intercepting k3s traffic:
# 10.42.0.0/16 dev tailscale0 
# 10.43.0.0/16 dev tailscale0
```

### Step 2: Verify Flannel is Working

```bash
# Check Flannel interface exists
ip link show flannel.1

# Check Flannel routes in main table
ip route | grep flannel

# Expected: Routes like "10.42.1.0/24 via 10.42.1.0 dev flannel.1"
```

### Step 3: Test Cross-Node Connectivity

```bash
# From a pod on node-2, try to reach a pod on node-1
kubectl exec -it -n swimto <api-pod> -- ping 10.42.0.x  # CoreDNS IP on node-1

# If this fails but same-node communication works, the routing conflict is confirmed
```

## Resolution

### Step 1: Remove Conflicting Routes (Immediate Fix)

```bash
# SSH to the affected node
ssh raolivei@node-2.eldertree.local

# Remove the conflicting routes from table 52
sudo ip route del 10.42.0.0/16 table 52 2>/dev/null
sudo ip route del 10.43.0.0/16 table 52 2>/dev/null

# Verify routes are removed
ip route show table 52 | grep -E "10\.4[23]"
# Should return nothing
```

### Step 2: Verify Connectivity

```bash
# Test DNS resolution from a pod
kubectl exec -it -n swimto <api-pod> -- nslookup google.com
kubectl exec -it -n swimto <api-pod> -- nslookup postgres-service.swimto.svc.cluster.local

# Test cross-node connectivity
kubectl exec -it -n swimto <api-pod> -- curl -s http://postgres-service.swimto:5432
```

### Step 3: Make the Fix Persistent

The routes will return after Tailscale or system restart. Create a systemd service:

```bash
# Create systemd service
cat <<EOF | sudo tee /etc/systemd/system/fix-tailscale-k3s-routes.service
[Unit]
Description=Fix Tailscale routes that conflict with k3s CNI
After=tailscaled.service k3s.service
Wants=tailscaled.service

[Service]
Type=oneshot
# Wait for Tailscale to fully initialize and add its routes
ExecStart=/bin/bash -c "sleep 30 && ip route del 10.42.0.0/16 table 52 2>/dev/null; ip route del 10.43.0.0/16 table 52 2>/dev/null"
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF

# Enable the service
sudo systemctl daemon-reload
sudo systemctl enable fix-tailscale-k3s-routes.service

# Test it works
sudo systemctl start fix-tailscale-k3s-routes.service
sudo systemctl status fix-tailscale-k3s-routes.service
```

### Step 4: Restart Affected Pods

```bash
# Restart pods that were failing
kubectl rollout restart deployment -n swimto swimto-api

# Verify they come up healthy
kubectl get pods -n swimto -w
```

## Alternative Solutions

### Option A: Tailscale ACLs (Preferred Long-term)

Configure Tailscale to not advertise routes for k3s CIDRs:

```json
{
  "acls": [
    // ... existing ACLs
  ],
  "autoApprovers": {
    "routes": {
      "10.42.0.0/16": [],  // Don't auto-approve pod network
      "10.43.0.0/16": []   // Don't auto-approve service network
    }
  }
}
```

### Option B: Change k3s CIDR

Use non-conflicting CIDRs for k3s:

```bash
# In k3s server config
--cluster-cidr=10.52.0.0/16
--service-cidr=10.53.0.0/16
```

**Note:** This requires cluster rebuild.

### Option C: Tailscale Exit Node Configuration

Exclude k3s CIDRs from Tailscale routing:

```bash
tailscale up --accept-routes=false
# Or configure split tunneling to exclude 10.42.0.0/16 and 10.43.0.0/16
```

## Network Architecture Reference

```
Nodes:
├── node-1: 192.168.2.101 (wlan0), pod CIDR 10.42.0.0/24
├── node-2: 192.168.2.102 (wlan0), pod CIDR 10.42.1.0/24  <- Usually affected
└── node-3: 192.168.2.103 (wlan0), pod CIDR 10.42.2.0/24

k3s Networks:
├── Pod Network: 10.42.0.0/16 (Flannel VXLAN)
└── Service Network: 10.43.0.0/16 (kube-proxy/iptables)

Routing Tables:
├── main: Normal system routing
├── 52: Tailscale policy routing <- Problem source
└── 254: Kernel local routing
```

## Prevention

1. **Monitor routing tables** after Tailscale updates or restarts
2. **Enable the systemd fix service** on all nodes with Tailscale
3. **Document Tailscale configuration** in cluster setup docs
4. **Consider Tailscale ACLs** to prevent route advertisement
5. **Test cross-node connectivity** after any network changes

## Verification Commands

```bash
# Check if fix is needed
ip route show table 52 | grep -E "10\.4[23]" && echo "FIX NEEDED" || echo "OK"

# Check systemd service status
systemctl status fix-tailscale-k3s-routes.service

# Check Flannel health
kubectl get pods -n kube-system -l k8s-app=flannel

# Check cross-node pod connectivity
kubectl run test --rm -it --image=busybox -- ping -c 3 <pod-ip-on-other-node>

# Check DNS resolution
kubectl run test --rm -it --image=busybox -- nslookup kubernetes.default.svc.cluster.local
```

## Related Issues

- NET-005: MetalLB VIP Not Responding (different root cause, similar symptoms)
- NODE-004: Node network isolation issues
- DNS-001: CoreDNS resolution failures

## References

- [Tailscale Subnet Routing](https://tailscale.com/kb/1019/subnets/)
- [k3s Networking](https://docs.k3s.io/networking)
- [Flannel VXLAN Backend](https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md)
- [Linux Policy Routing](https://man7.org/linux/man-pages/man8/ip-rule.8.html)

## Timeline of Discovery

**Date:** January 20, 2026
**Context:** SwimTO API deployment failing to connect to PostgreSQL

**Sequence of Events:**
1. API pods crashing with database connection timeout
2. Initial suspicion: DATABASE_URL misconfiguration
3. Changed to service name → DNS resolution failure
4. Tested same-node scheduling → worked perfectly
5. Investigated cross-node networking
6. Discovered Tailscale routing table 52 conflict
7. Removed conflicting routes → immediate fix
8. Created systemd service for persistence
9. Cluster fully operational

**Lesson Learned:** When cross-node communication fails but same-node works, check for VPN/overlay network conflicts in policy routing tables.
