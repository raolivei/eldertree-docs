---
id: HA-001
title: Node Failure and HA Cluster Recovery
category: ha
severity: critical
date: 2026-01-27
symptoms:
  - Node NotReady
  - Pods stuck in Terminating
  - Multi-Attach error for volume
  - Vault sealed after failover
  - ExternalSecrets sync errors
  - 502 errors on public sites
---

# HA-001: Node Failure and HA Cluster Recovery

## Incident Summary

**Date:** January 27, 2026

Node-1 went NotReady, triggering a cascade of issues across the 3-node HA cluster. This document covers the full recovery process and lessons learned.

## Error Messages (Searchable)

```
node/node-1.eldertree.local condition met: Ready=False
Multi-Attach error for volume "pvc-xxx" Volume is already attached to node node-1.eldertree.local
Cannot evict pod as it would violate the pod's disruption budget
502 Bad Gateway
Vault is sealed
SecretSyncedError
```

## Timeline

1. **Node-1 goes NotReady** - Hardware/network issue
2. **Pods stuck in Terminating** - Can't cleanly terminate on dead node
3. **Longhorn Multi-Attach errors** - Volumes still attached to dead node
4. **Vault failover issues** - vault-0 stuck, vault-1 becomes standby pointing to dead leader
5. **ExternalSecrets fail** - ClusterSecretStore pointing to old Vault IP
6. **Public sites return 502** - SSL certificate issues with Cloudflare "Full (strict)" mode

## Recovery Steps

### Step 1: Verify Cluster Quorum

```bash
export KUBECONFIG=~/.kube/config-eldertree
kubectl get nodes
# Confirm 2/3 nodes still Ready - quorum maintained
```

### Step 2: Force Delete Stuck Pods

```bash
# Find pods stuck in Terminating on the dead node
kubectl get pods -A -o wide | grep node-1 | grep Terminating

# Force delete (example for vault-0)
kubectl delete pod vault-0 -n vault --force --grace-period=0
```

### Step 3: Fix Longhorn Volume Attachments

```bash
# List stuck volumes
kubectl get volumes.longhorn.io -n longhorn-system

# Force detach volume from dead node
kubectl patch volumes.longhorn.io <volume-name> -n longhorn-system \
  --type=merge -p '{"spec":{"nodeID":""}}'

# Delete stuck volume attachment
kubectl delete volumeattachment <attachment-name>
```

### Step 4: Recover Vault HA

```bash
# Check Vault pod status
kubectl get pods -n vault

# If vault-0 recreated but sealed, unseal it
./scripts/operations/unseal-vault.sh

# Login with root token (from vault-unseal-keys secret)
ROOT_TOKEN=$(kubectl get secret vault-unseal-keys -n vault -o jsonpath='{.data.root-token}' | base64 -d)
kubectl exec -n vault vault-1 -- vault login $ROOT_TOKEN
```

### Step 5: Fix External Secrets Operator

```bash
# Restart ESO to pick up new Vault service IP
kubectl rollout restart deployment -n external-secrets

# Force sync ExternalSecrets
kubectl annotate externalsecret <name> -n <namespace> \
  force-sync=$(date +%s) --overwrite
```

### Step 6: Verify Services

```bash
# Test public endpoints
curl -I https://swimto.app
curl -I https://pitanga.cloud
curl -I https://swimto.eldertree.xyz
```

## Longhorn Configuration for HA

### Optimal Settings for 3-Node Cluster

```yaml
# 2 replicas (not 3) - allows 1 node to fail while staying healthy
defaultReplicaCount: "2"

# Node drain policy - allows draining when replicas exist elsewhere
nodeDrainPolicy: "block-if-contains-last-replica"

# Auto-delete pods when node goes down
nodeDownPodDeletionPolicy: "delete-both-statefulset-and-deployment-pod"

# Rebalance replicas when node comes back
replicaAutoBalance: "best-effort"
```

### Update Existing Volumes to 2 Replicas

```bash
# Change all volumes from 3 to 2 replicas
for vol in $(kubectl get volumes.longhorn.io -n longhorn-system -o jsonpath='{.items[*].metadata.name}'); do
  kubectl patch volumes.longhorn.io "$vol" -n longhorn-system \
    --type=merge -p '{"spec":{"numberOfReplicas":2}}'
done
```

### Why 2 Replicas is Better Than 3 for 3-Node Cluster

| Configuration | Node Fails | Result |
|--------------|------------|--------|
| 3 replicas on 3 nodes | 1 node down | **DEGRADED** (only 2/3 replicas) |
| 2 replicas on 3 nodes | 1 node down | **HEALTHY** (2/2 replicas exist) |

## PodDisruptionBudgets for Graceful Drains

### The Problem

Services with 1 replica and minAvailable=1 PDB cannot be evicted during drain.

### The Solution

Scale critical services to 2 replicas:

```bash
# Scale services
kubectl scale deployment traefik -n kube-system --replicas=2
kubectl scale deployment cloudflared -n cloudflare-tunnel --replicas=2
kubectl scale deployment external-dns -n external-dns --replicas=2
```

### After Scaling - PDBs Allow Disruptions

| Service | Replicas | PDB minAvailable | Allowed Disruptions |
|---------|----------|------------------|---------------------|
| traefik | 2 | 1 | 1 ✅ |
| cloudflared | 2 | 1 | 1 ✅ |
| external-dns | 2 | 1 | 1 ✅ |

## Bringing Node Back Online

```bash
# When node comes back
kubectl uncordon node-1.eldertree.local

# Re-enable Longhorn scheduling
kubectl patch nodes.longhorn.io node-1.eldertree.local -n longhorn-system \
  --type=merge -p '{"spec":{"allowScheduling":true}}'
```

## Prevention

1. **Keep Longhorn replicas at 2** for 3-node clusters
2. **Scale critical services to 2+ replicas** for graceful drains
3. **Regular Vault backups** - `./scripts/operations/backup-vault-secrets.sh`
4. **Monitor node health** with alerts
5. **Test failover regularly** - cordon/drain nodes periodically

## Related Runbooks

- [VAULT-001: Vault Recovery Guide](/runbook/issues/storage/VAULT-001)
- [CF-001: Cloudflare Tunnel Troubleshooting](/runbook/issues/cloudflare/CF-001)
- [LONGHORN-001: Longhorn Storage Issues](/runbook/issues/storage/LONGHORN-001)
